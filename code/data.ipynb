{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
    "label_mapping = {\n",
    "    'glioma_tumor': 0,\n",
    "    'meningioma_tumor': 1,\n",
    "    'no_tumor': 2,\n",
    "    'pituitary_tumor': 3\n",
    "}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/826 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 826/826 [00:00<00:00, 1369.48it/s]\n",
      "100%|██████████| 822/822 [00:00<00:00, 1438.31it/s]\n",
      "100%|██████████| 395/395 [00:00<00:00, 1354.30it/s]\n",
      "100%|██████████| 827/827 [00:00<00:00, 1255.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    path = f\"../data/Training/{label}\"\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        img_path = f\"{path}/{filename}\"\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (image_size, image_size))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            X_train.append(img)\n",
    "            y_train.append(label_mapping[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1199.60it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 1513.38it/s]\n",
      "100%|██████████| 105/105 [00:00<00:00, 2423.00it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 548.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    path = f\"../data/Testing/{label}\"\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        img_path = f\"{path}/{filename}\"\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (image_size, image_size))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            X_val.append(img)\n",
    "            y_val.append(label_mapping[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "\n",
    "\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.LocalWebserverAuth()\n",
    "# drive = GoogleDrive(gauth)\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# # Authenticate and create the PyDrive client\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.LocalWebserverAuth()  # For the first run, it will ask for your credentials\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "# # Save the files locally\n",
    "# datasets_dir = 'datasets/'\n",
    "\n",
    "# # List of files to save and upload\n",
    "# files_to_upload = [\n",
    "#     ('trainImages.pkl', X_train),\n",
    "#     ('trainLabels.pkl', y_train),\n",
    "#     ('valImages.pkl', X_val),\n",
    "#     ('valLabels.pkl', y_val)\n",
    "# ]\n",
    "\n",
    "# for filename, data in files_to_upload:\n",
    "#     filepath = os.path.join(datasets_dir, filename)\n",
    "#     with open(filepath, 'wb') as f:\n",
    "#         pickle.dump(data, f)\n",
    "#     print(f'Saved {filename} locally.')\n",
    "\n",
    "#     # Upload the .pkl file to Google Drive\n",
    "#     upload_file = drive.CreateFile({'title': filename})  # Set file name on Drive\n",
    "#     upload_file.SetContentFile(filepath)  # Set content from the local file\n",
    "#     upload_file.Upload()  # Upload the file\n",
    "#     print(f'Uploaded {filename} to Google Drive.')\n",
    "\n",
    "#     if os.path.exists(filepath):\n",
    "#         os.remove(filepath)\n",
    "#         print(f'Local file {filename} deleted.')\n",
    "#     else:\n",
    "#         print(f'Error: {filename} file not found.')\n",
    "\n",
    "# print(\"All files have been uploaded and deleted locally.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.0.0\n",
      "Dataset version v1.0.0 saved!\n"
     ]
    }
   ],
   "source": [
    "from versioning import save_data, get_next_version\n",
    "\n",
    "print(get_next_version(type=\"data\"))\n",
    "\n",
    "save_data(train_dataset, val_dataset, version=get_next_version(type=\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    elif torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 1st layer\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 256 / 2 = 128\n",
    "\n",
    "            # 2nd layer\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 128 / 2 = 64\n",
    "\n",
    "            # 3rd layer\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size= 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 64 / 2 = 32\n",
    "\n",
    "            # 4rd layer\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size= 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 32 / 2 = 16\n",
    "\n",
    "            nn.Flatten(), #256 * 16 * 16\n",
    "\n",
    "            # 1st linear\n",
    "            nn.Linear(256 * 16 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            # 2nd linear\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # 3nd linear\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            # 4rd linear\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 5th linear\n",
    "            nn.Linear(16, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimiser, criterion, epochs, device, writer, ckpt_path=\"../models/best.pt\"):\n",
    "    model.to(device)\n",
    "\n",
    "    best = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loop = (tqdm(enumerate(train_loader, 0), total= (len(train_loader)), desc=f\"Epoch {epoch}\"))  \n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "\n",
    "        for batch_idx, (feat, labels) in train_loop:\n",
    "            feat, labels = feat.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(feat)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step() \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "            train_loop.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "        if writer:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss / len(train_loader), epoch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct, total = 0, 0\n",
    "            model.eval()\n",
    "            val_loop = tqdm(enumerate(val_loader, 0), total=len(val_loader), desc=\"Val\")\n",
    "            for batch_idx, (feat, labels) in val_loop:\n",
    "                feat, labels = feat.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(feat)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                val_loop.set_postfix({\"acc\": correct / total})\n",
    "            \n",
    "            if writer:\n",
    "                writer.add_scalar(\"Accuracy/val\", correct / total, epoch)\n",
    "\n",
    "            if correct / total > best:\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                best = correct / total\n",
    "    return train_loss, correct / total \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 23/23 [00:08<00:00,  2.86it/s, loss=1.15]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00,  8.37it/s, acc=0.211]\n",
      "Epoch 1: 100%|██████████| 23/23 [00:07<00:00,  3.04it/s, loss=1.14] \n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 12.37it/s, acc=0.274]\n",
      "Epoch 2: 100%|██████████| 23/23 [00:07<00:00,  3.10it/s, loss=0.929]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 10.61it/s, acc=0.386]\n",
      "Epoch 3: 100%|██████████| 23/23 [00:07<00:00,  3.11it/s, loss=0.874]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 10.49it/s, acc=0.381]\n",
      "Epoch 4: 100%|██████████| 23/23 [00:07<00:00,  3.10it/s, loss=0.925]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 12.58it/s, acc=0.416]\n",
      "Epoch 5: 100%|██████████| 23/23 [00:07<00:00,  3.06it/s, loss=0.632]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 13.17it/s, acc=0.393]\n",
      "Epoch 6: 100%|██████████| 23/23 [00:07<00:00,  2.98it/s, loss=0.388]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00,  9.29it/s, acc=0.444]\n",
      "Epoch 7: 100%|██████████| 23/23 [00:07<00:00,  2.94it/s, loss=0.412]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 11.73it/s, acc=0.414]\n",
      "Epoch 8: 100%|██████████| 23/23 [00:07<00:00,  3.05it/s, loss=0.538]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 13.36it/s, acc=0.467]\n",
      "Epoch 9: 100%|██████████| 23/23 [00:07<00:00,  2.98it/s, loss=0.764]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 13.83it/s, acc=0.457]\n",
      "Epoch 10: 100%|██████████| 23/23 [00:07<00:00,  3.07it/s, loss=0.292]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00,  9.24it/s, acc=0.51] \n",
      "Epoch 11: 100%|██████████| 23/23 [00:07<00:00,  3.01it/s, loss=0.283]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00,  8.42it/s, acc=0.553]\n",
      "Epoch 12: 100%|██████████| 23/23 [00:07<00:00,  2.96it/s, loss=0.283]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 10.55it/s, acc=0.581]\n",
      "Epoch 13: 100%|██████████| 23/23 [00:07<00:00,  3.06it/s, loss=0.25] \n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 11.43it/s, acc=0.515]\n",
      "Epoch 14: 100%|██████████| 23/23 [00:07<00:00,  2.99it/s, loss=0.175]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 10.42it/s, acc=0.553]\n",
      "Epoch 15: 100%|██████████| 23/23 [00:07<00:00,  3.10it/s, loss=0.205]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 10.67it/s, acc=0.68] \n",
      "Epoch 16: 100%|██████████| 23/23 [00:07<00:00,  2.89it/s, loss=0.238]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00,  8.48it/s, acc=0.576]\n",
      "Epoch 17: 100%|██████████| 23/23 [00:07<00:00,  2.99it/s, loss=0.132]\n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 11.69it/s, acc=0.708]\n",
      "Epoch 18: 100%|██████████| 23/23 [00:07<00:00,  3.04it/s, loss=0.337] \n",
      "Val: 100%|██████████| 4/4 [00:00<00:00, 12.80it/s, acc=0.698]\n",
      "Epoch 19: 100%|██████████| 23/23 [00:07<00:00,  2.96it/s, loss=0.102] \n",
      "Val: 100%|██████████| 4/4 [00:00<00:00,  9.31it/s, acc=0.736]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = CNN()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = select_device()\n",
    "epochs = 20\n",
    "\n",
    "train_loss, val_acc = train(model, train_loader, val_loader, optimiser, loss_fn, epochs, device, SummaryWriter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.0.1\n"
     ]
    }
   ],
   "source": [
    "from versioning import save_model, get_next_version\n",
    "\n",
    "print(get_next_version())\n",
    "\n",
    "save_model(model, optimiser, epoch=20, loss=0.102, accuracy=0.736, version='v1.0.0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    data = data.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        model.to(\"cpu\")\n",
    "\n",
    "        outputs = model(data)\n",
    "        \n",
    "        _, label = torch.max(outputs.data, 1)\n",
    "    return label.item()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_train[900], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
